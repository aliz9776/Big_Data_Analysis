{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XHbFpNHh7pOT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "creditcard_df = pd.read_csv('creditcard.csv' )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess and quantile-based discretization and then convert to catgorical variable"
      ],
      "metadata": {
        "id": "mMH9boJtL8FJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7vgpVB6Xgaw",
        "outputId": "2c01920a-1ff4-4fad-a9f6-8420687fd751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Class  V1_bin_0  V1_bin_1  V1_bin_2  V1_bin_3  V1_bin_4  V1_bin_5  \\\n",
            "0    0.0         0         0         0         1         0         0   \n",
            "1    0.0         0         0         0         0         0         0   \n",
            "2    0.0         0         0         0         1         0         0   \n",
            "3    0.0         0         0         0         0         0         1   \n",
            "4    0.0         0         0         0         0         1         0   \n",
            "\n",
            "   V1_bin_6  V1_bin_7  V1_bin_8  ...  Amount_bin_10  Amount_bin_11  \\\n",
            "0         0         0         0  ...              0              0   \n",
            "1         0         0         0  ...              0              0   \n",
            "2         0         0         0  ...              0              0   \n",
            "3         0         0         0  ...              0              0   \n",
            "4         0         0         0  ...              0              0   \n",
            "\n",
            "   Amount_bin_12  Amount_bin_13  Amount_bin_14  Amount_bin_15  Amount_bin_16  \\\n",
            "0              0              0              0              0              1   \n",
            "1              0              0              0              0              0   \n",
            "2              0              0              0              0              0   \n",
            "3              0              0              0              0              1   \n",
            "4              0              0              1              0              0   \n",
            "\n",
            "   Amount_bin_17  Amount_bin_18  Amount_bin_19  \n",
            "0              0              0              0  \n",
            "1              0              0              0  \n",
            "2              0              1              0  \n",
            "3              0              0              0  \n",
            "4              0              0              0  \n",
            "\n",
            "[5 rows x 581 columns]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "import numpy as np\n",
        "\n",
        "# data pre-processing\n",
        "\n",
        "# Handle missing values\n",
        "creditcard_df.dropna(inplace=True)  # Drop rows with missing values\n",
        "\n",
        "\n",
        "# Drop the \"Time\" column because it is unnecessary\n",
        "creditcard_df.drop(columns='Time', inplace=True)\n",
        "\n",
        "# Use quantile-based discretization\n",
        "discretizer = KBinsDiscretizer(n_bins=20,  strategy='quantile')  # Use quantile-based discretization\n",
        "\n",
        "# Specify the numerical columns to be discretized\n",
        "num_cols = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
        "\n",
        "# Perform data discretization for each numerical feature\n",
        "# Loop through each numerical column and discretize\n",
        "for col in num_cols:\n",
        "    # Extract the column values as a 2D numpy array\n",
        "    col_values = creditcard_df[col].values.reshape(-1, 1)\n",
        "    \n",
        "    # Apply KBinsDiscretizer and get the transformed feature\n",
        "    discretized_feature = discretizer.fit_transform(col_values)\n",
        "    \n",
        "    # Convert the sparse matrix to a dense numpy array\n",
        "    discretized_feature = discretized_feature.toarray()\n",
        "    \n",
        "    # Create new column names for the discretized feature\n",
        "    col_names = [col + '_bin_' + str(i) for i in range(discretized_feature.shape[1])]\n",
        "    \n",
        "    # Create a DataFrame from the discretized feature\n",
        "    discretized_df = pd.DataFrame(discretized_feature.astype(int), columns=col_names)\n",
        "    \n",
        "    # Concatenate the discretized_df to creditcard_df\n",
        "    creditcard_df = pd.concat([creditcard_df, discretized_df], axis=1)\n",
        "\n",
        "# Drop the original numerical columns\n",
        "creditcard_df.drop(columns=num_cols, inplace=True)\n",
        "\n",
        "# Display the updated creditcard_df\n",
        "print(creditcard_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc8hPXOsxR_K",
        "outputId": "f0ea6163-b7ab-403b-a444-0fe305f60c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, leverage, conviction]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules\n",
        "\n",
        "\n",
        "# Determine the minimum and maximum frequency values within the desired range\n",
        "min_freq = 400\n",
        "max_freq = 1000\n",
        "min_support = min_freq / len(creditcard_df)\n",
        "max_support = max_freq / len(creditcard_df)\n",
        "\n",
        "# Define the frequent itemsets using Apriori algorithm\n",
        "frequent_itemsets = apriori(creditcard_df, min_support=min_support, use_colnames=True)\n",
        "\n",
        "# Filter the frequent itemsets based on length of 3 or 4\n",
        "frequent_itemsets = frequent_itemsets[(frequent_itemsets['itemsets'].apply(len) == 3) | (frequent_itemsets['itemsets'].apply(len) == 4)]\n",
        "\n",
        "# Filter the frequent itemsets based on support threshold\n",
        "frequent_itemsets = frequent_itemsets[(frequent_itemsets['support'] >= min_support) & (frequent_itemsets['support'] <= max_support)]\n",
        "\n",
        "# Generate the association rules from the frequent itemsets (only support values)\n",
        "rules = association_rules(frequent_itemsets, metric=\"support\", support_only=True)\n",
        "\n",
        "# Filter the rules based on left side length of 2 or 3\n",
        "rules = rules[(rules['antecedents'].apply(len) == 2) | (rules['antecedents'].apply(len) == 3)]\n",
        "\n",
        "# Filter the rules based on right side being the desired class\n",
        "rules = rules[rules['consequents'] == frozenset({'Class_1'})]\n",
        "\n",
        "# Print the filtered association rules\n",
        "print(rules)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bwlRNWAX1axn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "d99fb7eb-47ae-492e-b412-28125bbd2680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, leverage, conviction]\n",
            "Index: []\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'antecedents'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0d5e260f83a1>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Calculate confidence and interest for rules_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mantecedents_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrules_3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'antecedents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mconfidence_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minterest_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'antecedents'"
          ]
        }
      ],
      "source": [
        "print(rules.head())\n",
        "# Generate association rules with support only\n",
        "rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01, support_only=True)\n",
        "\n",
        "# Filter rules with antecedent length of 3 or 4\n",
        "rules_3 = rules[rules['antecedents'].apply(lambda x: len(x) == 3)]\n",
        "rules_4 = rules[rules['antecedents'].apply(lambda x: len(x) == 4)]\n",
        "\n",
        "# Calculate confidence and interest for rules_3\n",
        "antecedents_3 = rules_3['antecedents'].apply(lambda x: frozenset(x))\n",
        "confidence_3 = []\n",
        "interest_3 = []\n",
        "for antecedent in antecedents_3:\n",
        "    support_antecedent = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]\n",
        "    support_rule = rules_3[rules_3['antecedents'].apply(lambda x: frozenset(x)) == antecedent]['support'].values[0]\n",
        "    confidence = support_rule / support_antecedent\n",
        "    interest = confidence / support_antecedent\n",
        "    confidence_3.append(confidence)\n",
        "    interest_3.append(interest)\n",
        "rules_3['confidence'] = confidence_3\n",
        "rules_3['interest'] = interest_3\n",
        "\n",
        "# Calculate confidence and interest for rules_4\n",
        "antecedents_4 = rules_4['antecedents'].apply(lambda x: frozenset(x))\n",
        "confidence_4 = []\n",
        "interest_4 = []\n",
        "for antecedent in antecedents_4:\n",
        "    support_antecedent = frequent_itemsets[frequent_itemsets['itemsets'] == antecedent]['support'].values[0]\n",
        "    support_rule = rules_4[rules_4['antecedents'].apply(lambda x: frozenset(x)) == antecedent]['support'].values[0]\n",
        "    confidence = support_rule / support_antecedent\n",
        "    interest = confidence / support_antecedent\n",
        "    confidence_4.append(confidence)\n",
        "    interest_4.append(interest)\n",
        "rules_4['confidence'] = confidence_4\n",
        "rules_4['interest'] = interest_4\n",
        "\n",
        "# Sort rules by confidence and interest\n",
        "top_5_rules_3_confidence = rules_3.sort_values(by='confidence', ascending=False).head(5)\n",
        "top_5_rules_3_interest = rules_3.sort_values(by='interest', ascending=False).head(5)\n",
        "\n",
        "top_5_rules_4_confidence = rules_4.sort_values(by='confidence', ascending=False).head(5)\n",
        "top_5_rules_4_interest = rules_4.sort_values(by='interest', ascending=False).head(5)\n",
        "\n",
        "# Print the results\n",
        "print(\"Top 5 rules with highest confidence (3 features):\")\n",
        "print(top_5_rules_3_confidence[['antecedents', 'consequents', 'confidence']])\n",
        "print(\"\\nTop 5 rules with highest interest (3 features):\")\n",
        "print(top_5_rules_3_interest[['antecedents', 'consequents', 'interest']])\n",
        "\n",
        "print(\"\\nTop 5 rules with highest confidence (4 features):\")\n",
        "print(top_5_rules_4_confidence[['antecedents', 'consequents', 'confidence']])\n",
        "print(\"\\nTop 5 rules with highest interest (4 features):\")\n",
        "print(top_5_rules_4_interest[['antecedents', 'consequents', 'interest']])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}